{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze A/B Test Results\n",
    "\n",
    "\n",
    "For this Udacity project, I run A/B testing on data from a fictious e-commerce website to test the efficacy of a new webpage design. \n",
    "\n",
    "A/B testing is essentially running a randomized control trial (econ lingo) and using hypothesis testing to see if the treatment group (new webpage) is more effective than the control group (original web page). \n",
    "\n",
    "I first manually find the p-value that shows there is not a statistically significant difference between the new and original pages.\n",
    "Then, I use the statsmodels.api built-in to produce the same result.\n",
    "After, I run a logistic regression to validate my previous results.\n",
    "Lastly, I merge in a dataset on user's location and merge it with the original dataframe and run another regression with interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Setting the seed to assure the same numbers on future executions of this code\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>2017-01-21 22:11:48.556739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>2017-01-12 08:01:45.159739</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>2017-01-21 01:52:26.210827</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "0   851104  2017-01-21 22:11:48.556739    control     old_page          0\n",
       "1   804228  2017-01-12 08:01:45.159739    control     old_page          0\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "4   864975  2017-01-21 01:52:26.210827    control     old_page          1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the dataset and inspect the first few rows\n",
    "df = pd.read_csv(\"ab_data.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I examine the new data with .head(), which shows me the first five observations. I see there are 5 column/variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "294478"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use .shape[0] to find the number of rows. .shape[] will return the ordered pair (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290584"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use .nunique() to find the user_id that are unique. It appears that 290,584 / 294,478 observations come from a unique user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12104245244060237"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converted = df[df['converted']==1].user_id.nunique() / df.user_id.nunique()\n",
    "converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use only unique users to computer the proportion of users converted to avoid repeats. The conversion rate for the entire dataset is ~12.1%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3893"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['group']==\"treatment\") & (df['landing_page']==\"old_page\")].shape[0] \\\n",
    "    + \\\n",
    "    df[(df['group']==\"control\") & (df['landing_page']==\"new_page\")].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check to see if there are any errors in that data. The 'treatment' is exposing the user to the 'new_page'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It appears that 3,893 observations in the dataset are coded incorrectly and should be dropped. I find this by checking for orbservations in the 'treatment' group seeing 'old page' or the 'control group' seeing 'new page'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294478 entries, 0 to 294477\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294478 non-null  int64 \n",
      " 1   timestamp     294478 non-null  object\n",
      " 2   group         294478 non-null  object\n",
      " 3   landing_page  294478 non-null  object\n",
      " 4   converted     294478 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I check for missing values using .info(), which returns the number of non-missing observations for each column/variable. Of the 294,478 observations, all 5 of the column/variables have 294,478 observations. Therefore, there are no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>2017-01-11 16:55:06.154213</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>2017-01-08 18:28:03.143765</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>679687</td>\n",
       "      <td>2017-01-19 03:26:46.940749</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>817355</td>\n",
       "      <td>2017-01-04 17:58:08.979471</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>839785</td>\n",
       "      <td>2017-01-15 18:11:06.610965</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                   timestamp      group landing_page  converted\n",
       "2   661590  2017-01-11 16:55:06.154213  treatment     new_page          0\n",
       "3   853541  2017-01-08 18:28:03.143765  treatment     new_page          0\n",
       "6   679687  2017-01-19 03:26:46.940749  treatment     new_page          1\n",
       "8   817355  2017-01-04 17:58:08.979471  treatment     new_page          1\n",
       "9   839785  2017-01-15 18:11:06.610965  treatment     new_page          1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I remove the observations that may be coding errors by creating two subset dataframes: 'treatment' and 'control'.\n",
    "treatment= df[(df['group']==\"treatment\") & (df['landing_page']==\"new_page\")]\n",
    "control = df[(df['group']==\"control\") & (df['landing_page']==\"old_page\")]\n",
    "\n",
    "# I them append (concatenate) one to the other and rename it df2. I continue the analysis for the project with df2.\n",
    "df_test=pd.concat([treatment,control])\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I verify all of the correct rows were removed by re-running my previous code and ensuring it returns 0 observations\n",
    "df2[((df2['group'] == 'treatment') == (df2['landing_page'] == 'new_page')) == False].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290584, 290585)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check the number of unique users in the new dataframe\n",
    "df2.user_id.nunique() , df2.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 290,584 unique users in df2 out of a total of 290,585 users. Thus, there is 1 user repeated in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2893    773192\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I use .duplicated() and specify to find duplicate rows based on column/variable 'user_id'. \n",
    "dup = df2[df2.duplicated(['user_id'])]['user_id']\n",
    "dup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one 'user_id' that is repeated twice in df2 is #773192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-09 05:37:58.781806</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>773192</td>\n",
       "      <td>2017-01-14 02:55:59.590927</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id                   timestamp      group landing_page  converted\n",
       "1899   773192  2017-01-09 05:37:58.781806  treatment     new_page          0\n",
       "2893   773192  2017-01-14 02:55:59.590927  treatment     new_page          0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I check the row information for the repeat user_id \n",
    "df2[df2['user_id']==773192]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears the duplicate user visited the new_page twice, on 2017-01-09 and 2017-01-14."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I drop one of the two repeated user_id so that the dataset is composed of only unique observations one visit per user_id). \n",
    "df2=df2.drop([2893])\n",
    "# I confirm the drop was successful by verifying that the number of unique observations in the data frame is equal to the \n",
    "# rows/observations. \n",
    "df2.user_id.nunique() == df2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I find the probability of an individual converting regardless of the page they receive by taking the proportion of\n",
    "# converted individuals relative to the number of individuals in the entire dataframe.\n",
    "overall_cr = len(df2.query('converted == 1')) / len(df2)\n",
    "overall_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of conversion among the entire dataframe is 11.96%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1203863045004612"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I find the probability of being converted given that an individual is in the 'control' grup by taking the \n",
    "# proportion of converted individuals in the control group relative to the total number of individuals in the control group.\n",
    "control_cr = len(df2.query('converted == 1 & group == \"control\"')) / len(df2.query('group == \"control\"'))\n",
    "control_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of conversion conditional on being in the control group is ~12.04%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I find the probability of being converted given that an individual is in the treatment group by taking the proportion of \n",
    "# converted individuals in the treatment group relative to the total number of individuals in the treatment group.\n",
    "treatment_cr = len(df2.query('converted == 1 & group == \"treatment\"')) / len(df2.query('group == \"treatment\"'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of conversion conditional on being in the treatment group is 11.88%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000619442226688"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I find the probability that an individual received the new page by taking the proportion of individuals with 'landing_page' \n",
    "# as 'new_page' relative to the total number of individuals in the entire data frame.\n",
    "new_page = len(df2.query('landing_page == \"new_page\"'))/ len(df2)\n",
    "new_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of an individual receiving the new page is approximately 50%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the descriptive statistics above, I do not believe there is sufficient evidence to conclude that the new treatment page leads to more conversions. We can observe that there is a lower proportion of conversions among the treatment group than there is for the control group. This suggests the new page may not be as effective. However, we should conduct a hypothesis test to make statistical inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ab_test'></a>\n",
    "### Part II - A/B Test\n",
    "\n",
    "I can set up a hypothesis test to see if the new page is signficantly better than the old page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$H_{o}$** : **$p_{new}$** - **$p_{old}$** <= 0 \n",
    "<br> **$H_{a}$** : **$p_{new}$** - **$p_{old}$** > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will first perform the hypothesis test step-by-step to demonstrate conceputal understanding and then use a simple pre-made function.\n",
    "\n",
    "To empirically test this, I will draw random samples from the new (treated) and old (control) pages and get the sampling\n",
    "difference 10,000 times.\n",
    "\n",
    "The probability of conversion among the entire dataframe is 11.96%. \n",
    "I found this by taking the proportion of converted individuals relative to the number of individuals in the entire dataframe\n",
    "in previous codes above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11959708724499628"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability of conversion among the entire dataframe is 11.96%. \n",
    "I found this by taking the proportion of converted individuals relative to the number of individuals in the entire dataframe.\n",
    "We assume that under the null hypothesis, the conversion rate for the new page is no bigger than that of the old page. That is, the null is assuming there in no effect or that the new page is less effective at than the old page in terms of conversion rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145310"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of individuals in the treatment group is below\n",
    "new_size = df2.query('group == \"treatment\"').shape[0]\n",
    "new_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145274"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of individuals in the control group is below\n",
    "old_size = df2.query('group == \"control\"').shape[0]\n",
    "old_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I simulate the new transactions with a conversion rate the same as that for the overall conversion rate under the null, and store them as binary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11968205904617714"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page_converted = random.choices([0,1], weights=[1-overall_cr, overall_cr], k=new_size)\n",
    "new_page_converted = np.array(new_page_converted)\n",
    "new_page_converted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11987692222971763"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I do the same as above, but for the old page conversion (control group)\n",
    "old_page_converted = random.choices([0,1], weights=[1-overall_cr, overall_cr], k=old_size)\n",
    "old_page_converted = np.array(old_page_converted)\n",
    "old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0010895414546477633"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_page_converted.mean() - old_page_converted.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now run a simulation by randomly generating the datasets for new_page and old_page sample sizes with each having the likelihood of a converstion identical to the total dataset (new and old page combined). I chose to use a binomial function, setting the probability of success (conversion) equal to that of the overall dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_diffs = []\n",
    "new_converted_sim = np.random.binomial(new_size, overall_cr, 10000)/new_size\n",
    "old_converted_sim = np.random.binomial(old_size, overall_cr, 10000)/new_size\n",
    "p_diffs = new_converted_sim - old_converted_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# I convert the list generated in the simulation to an numpy array and plot it. \n",
    "p_diffs = np.array(p_diffs)\n",
    "plt.hist(p_diffs);\n",
    "\n",
    "# I put a red line where the actual difference observed in the ab_data.csv occurs (the actual experiment). \n",
    "plt.axvline(x=(treatment_cr - control_cr), color='red');\n",
    "# This line falls to the left of the median for the randomly generated draws. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(p_diffs > (treatment_cr - control_cr)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have calculated the 'p-value', which is the likelihood that we incorrectly reject the null hypothesis. \n",
    "Typically, practioners are willing to accidently reject the null 5% of the time. The p-value of 0.91 suggests that there is not statistically significant evidence that the new page led to a higher conversion rate than the old page. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I now use a built-in to achieve similar results. \n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Calculate the numer of conversions in each group\n",
    "convert_old = len(df2.query('converted == 1 & group == \"control\"'))\n",
    "convert_new = len(df2.query('converted == 1 & group == \"treatment\"'))\n",
    "# Calcuate the number of individuals who received each page\n",
    "n_old = len(df2.query('group == \"control\"'))\n",
    "n_new = len(df2.query('group == \"treatment\"'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.3109241984234394, 0.9050583127590245)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I use `stats.proportions_ztest` to compute the test statistic and p-value.  \n",
    "z_score, p_value = sm.stats.proportions_ztest([convert_old, convert_new], [n_old, n_new],  alternative = 'smaller')\n",
    "z_score, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critical value for a z-score with type 1 error of 5% is 1.64. The z-score from this z-test is 1.31, which is not large enough to reject the null hypothesis. This result is consistent with the manual finding above, that there is not statistical evidence that the new page led to higher conversions that the old."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part III - Regression approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now show that the same result achieved above can be also found by performing regression. \n",
    "Since each row is either a conversion or no conversion, a binary outcome, I use a logistic regression.\n",
    "\n",
    "I use statsmodels to fit the regression model I specified above to see if there is a significant difference \n",
    "in conversion based on which page a customer receives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I define a new column for the intercept\n",
    "df2['intercept']=1\n",
    "# I create a dummy variable column for which page each user received, which is 1 when an individual receives the \n",
    "# **treatment** and 0 if **control**\n",
    "df2[['no_page', 'ab_page']]=pd.get_dummies(df['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366118\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212780.3502</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2025-02-19 15:57</td>       <td>BIC:</td>        <td>212801.5095</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>1</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290582</td>        <td>LLR p-value:</td>      <td>0.18988</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9888</td>  <td>0.0081</td>  <td>-246.6690</td> <td>0.0000</td> <td>-2.0046</td> <td>-1.9730</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0150</td>  <td>0.0114</td>   <td>-1.3109</td>  <td>0.1899</td> <td>-0.0374</td> <td>0.0074</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212780.3502\n",
       "Date:               2025-02-19 15:57 BIC:              212801.5095\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           1                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290582           LLR p-value:      0.18988    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9888    0.0081  -246.6690  0.0000  -2.0046  -1.9730\n",
       "ab_page      -0.0150    0.0114    -1.3109  0.1899  -0.0374   0.0074\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I instantiate the regression model on the two columns that I just created, then fit the model \n",
    "# to predict whether or not an individual converts. \n",
    "log_m = sm.Logit(df2['converted'], df2[['intercept', 'ab_page']])\n",
    "results = log_m.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the regression, the null assumes that the average effect of viewing the new page on conversion is zero. The alternative is that it is not equal to zero. \n",
    "This is slightly different than was done in Part II, where the null assumed the old page is better unless the new page proves to be definitely better (less than or equal to rather than just equal to). The alternative hypothesis was that the new page is strictly better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering other factors that might influence conversion in the same model help mitigate potential omitted variable bias. \n",
    "For example, there may be a variable currently omitted from the model that is correlated with both seeing the page and conversion. \n",
    "When this variable is omitted, the coefficient on ab_page is biased. In order to uncover the true effect of ab_page on conversion, we need to add this omitted variable to the model to hold its effect on conversion constant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I leverage a supplemental dataset of user's location by country to see if that has an impact on the liklihood of conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "US    203619\n",
       "UK     72466\n",
       "CA     14499\n",
       "Name: country, dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I read in the new dataframe\n",
    "country_df = pd.read_csv('countries.csv')\n",
    "country_df.country.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears there are three countries that various observations live in: United States, Canada, and United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I create dummy variables accordingly below.\n",
    "country_df[['CA', 'UK', 'US']] = pd.get_dummies(country_df['country'])\n",
    "\n",
    "# I  merge the datasets using user_id to link them together\n",
    "df3 = pd.merge(df2, country_df, on=\"user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new dataframe that includes country information on the observations, I will run a new logistic regression to see the effect. I will have US be the base group that the comparisons will be made on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366113\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212781.1253</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2025-02-19 16:02</td>       <td>BIC:</td>        <td>212823.4439</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>3</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290580</td>        <td>LLR p-value:</td>      <td>0.17599</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th> <td>-1.9794</td>  <td>0.0127</td>  <td>-155.4145</td> <td>0.0000</td> <td>-2.0044</td> <td>-1.9544</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>   <td>-0.0149</td>  <td>0.0114</td>   <td>-1.3069</td>  <td>0.1912</td> <td>-0.0374</td> <td>0.0075</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>        <td>-0.0506</td>  <td>0.0284</td>   <td>-1.7835</td>  <td>0.0745</td> <td>-0.1063</td> <td>0.0050</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>        <td>-0.0099</td>  <td>0.0133</td>   <td>-0.7433</td>  <td>0.4573</td> <td>-0.0359</td> <td>0.0162</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212781.1253\n",
       "Date:               2025-02-19 16:02 BIC:              212823.4439\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           3                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290580           LLR p-value:      0.17599    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9794    0.0127  -155.4145  0.0000  -2.0044  -1.9544\n",
       "ab_page      -0.0149    0.0114    -1.3069  0.1912  -0.0374   0.0075\n",
       "CA           -0.0506    0.0284    -1.7835  0.0745  -0.1063   0.0050\n",
       "US           -0.0099    0.0133    -0.7433  0.4573  -0.0359   0.0162\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_m2 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'CA', 'US']])\n",
    "results = log_m2.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not a statistical difference between country and the odds of conversion in this dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now look at an interaction between page and country to see if there significant effects on conversion.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I Create the necessary additional columns\n",
    "df3['ab_page_CA']=df3['ab_page']*df3['CA']\n",
    "df3['ab_page_US']=df3['ab_page']*df3['US']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.366109\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td>Model:</td>              <td>Logit</td>      <td>Pseudo R-squared:</td>    <td>0.000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <td>Dependent Variable:</td>     <td>converted</td>          <td>AIC:</td>        <td>212782.6602</td>\n",
       "</tr>\n",
       "<tr>\n",
       "         <td>Date:</td>        <td>2025-02-19 16:03</td>       <td>BIC:</td>        <td>212846.1381</td>\n",
       "</tr>\n",
       "<tr>\n",
       "   <td>No. Observations:</td>       <td>290584</td>       <td>Log-Likelihood:</td>  <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "       <td>Df Model:</td>              <td>5</td>            <td>LL-Null:</td>      <td>-1.0639e+05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "     <td>Df Residuals:</td>         <td>290578</td>        <td>LLR p-value:</td>      <td>0.19199</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "      <td>Converged:</td>           <td>1.0000</td>           <td>Scale:</td>         <td>1.0000</td>   \n",
       "</tr>\n",
       "<tr>\n",
       "    <td>No. Iterations:</td>        <td>6.0000</td>              <td></td>               <td></td>      \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>       <th>Coef.</th>  <th>Std.Err.</th>     <th>z</th>      <th>P>|z|</th> <th>[0.025</th>  <th>0.975]</th> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>intercept</th>  <td>-1.9922</td>  <td>0.0161</td>  <td>-123.4571</td> <td>0.0000</td> <td>-2.0238</td> <td>-1.9606</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page</th>    <td>0.0108</td>   <td>0.0228</td>   <td>0.4749</td>   <td>0.6349</td> <td>-0.0339</td> <td>0.0555</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CA</th>         <td>-0.0118</td>  <td>0.0398</td>   <td>-0.2957</td>  <td>0.7674</td> <td>-0.0899</td> <td>0.0663</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>US</th>         <td>0.0057</td>   <td>0.0188</td>   <td>0.3057</td>   <td>0.7598</td> <td>-0.0311</td> <td>0.0426</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page_CA</th> <td>-0.0783</td>  <td>0.0568</td>   <td>-1.3783</td>  <td>0.1681</td> <td>-0.1896</td> <td>0.0330</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ab_page_US</th> <td>-0.0314</td>  <td>0.0266</td>   <td>-1.1807</td>  <td>0.2377</td> <td>-0.0835</td> <td>0.0207</td> \n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary2.Summary'>\n",
       "\"\"\"\n",
       "                          Results: Logit\n",
       "==================================================================\n",
       "Model:              Logit            Pseudo R-squared: 0.000      \n",
       "Dependent Variable: converted        AIC:              212782.6602\n",
       "Date:               2025-02-19 16:03 BIC:              212846.1381\n",
       "No. Observations:   290584           Log-Likelihood:   -1.0639e+05\n",
       "Df Model:           5                LL-Null:          -1.0639e+05\n",
       "Df Residuals:       290578           LLR p-value:      0.19199    \n",
       "Converged:          1.0000           Scale:            1.0000     \n",
       "No. Iterations:     6.0000                                        \n",
       "-------------------------------------------------------------------\n",
       "              Coef.   Std.Err.      z      P>|z|    [0.025   0.975]\n",
       "-------------------------------------------------------------------\n",
       "intercept    -1.9922    0.0161  -123.4571  0.0000  -2.0238  -1.9606\n",
       "ab_page       0.0108    0.0228     0.4749  0.6349  -0.0339   0.0555\n",
       "CA           -0.0118    0.0398    -0.2957  0.7674  -0.0899   0.0663\n",
       "US            0.0057    0.0188     0.3057  0.7598  -0.0311   0.0426\n",
       "ab_page_CA   -0.0783    0.0568    -1.3783  0.1681  -0.1896   0.0330\n",
       "ab_page_US   -0.0314    0.0266    -1.1807  0.2377  -0.0835   0.0207\n",
       "==================================================================\n",
       "\n",
       "\"\"\""
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the new model. \n",
    "log_m3 = sm.Logit(df3['converted'], df3[['intercept', 'ab_page', 'CA', 'US', 'ab_page_CA', 'ab_page_US']])\n",
    "results = log_m3.fit()\n",
    "results.summary2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I run the model using interaction terms between page and country. It appears that there is not a statistically significant effect of viewing the new page for people in Canada or United States relative to viewing the page in the United Kingdom. Further, there is no statisically significant relationship overall between a person's country and their likelihood of conversion. In conculsion, we cannot reject the null hypothesis that the new page is associated with a different amount of conversions than the old page. In other words, there is no statistical difference between viewing the new page or the old page on the likelihood of a person's conversion. \n",
    "\n",
    "After reviewing the two different tests performed in part 2 (failure to reject the hypothesis that the new page is no better than the old page) and part 3 (failure to reject the hypothesis that the new page is the same as the old page), I conclude the average effect of both pages on conversion is statistically the same. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
